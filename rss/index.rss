<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>TBD</title><description>Thoughts, stories and ideas.</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>TBD</title><link>http://localhost:2368/</link></image><generator>Ghost 2.2</generator><lastBuildDate>Thu, 25 Oct 2018 06:18:54 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Auction: Vickrey–Clarke–Groves (VCG) 
 or Generalized Second Price (GSP)?</title><description>&lt;p&gt;test&lt;/p&gt;</description><link>http://localhost:2368/generalized-second-price-auction/</link><guid isPermaLink="false">5bd15d1f1258b83180412192</guid><category>Random</category><dc:creator>Chenglin Lu</dc:creator><pubDate>Thu, 25 Oct 2018 06:16:57 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/10/rawpixel-1055781-unsplash.jpg" medium="image"/><content:encoded>&lt;img src="http://localhost:2368/content/images/2018/10/rawpixel-1055781-unsplash.jpg" alt="Auction: Vickrey–Clarke–Groves (VCG) 
 or Generalized Second Price (GSP)?"&gt;&lt;p&gt;test&lt;/p&gt;</content:encoded></item><item><title>The Multi-Armed Bandit Problem</title><description>&lt;h2 id="problem-setup"&gt;Problem Setup&lt;/h2&gt;&lt;p&gt;The multi-armed bandit problem can be described as a Markov decision process, a tuple $\langle \mathcal{X}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, with only one state.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;$\mathcal{X} = {x}$ is a finite set of states&lt;/li&gt;&lt;li&gt;$\mathcal{A}$ is a finite set of actions&lt;/li&gt;&lt;li&gt;$\mathcal{P}&lt;/li&gt;&lt;/ul&gt;</description><link>http://localhost:2368/test/</link><guid isPermaLink="false">5bcc1aabf10170222eac70fa</guid><category>Reinforcement Learning</category><dc:creator>Chenglin Lu</dc:creator><pubDate>Sun, 21 Oct 2018 06:20:35 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/10/carl-raw-584973-unsplash.jpg" medium="image"/><content:encoded>&lt;h2 id="problem-setup"&gt;Problem Setup&lt;/h2&gt;&lt;img src="http://localhost:2368/content/images/2018/10/carl-raw-584973-unsplash.jpg" alt="The Multi-Armed Bandit Problem"&gt;&lt;p&gt;The multi-armed bandit problem can be described as a Markov decision process, a tuple $\langle \mathcal{X}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, with only one state.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;$\mathcal{X} = {x}$ is a finite set of states&lt;/li&gt;&lt;li&gt;$\mathcal{A}$ is a finite set of actions&lt;/li&gt;&lt;li&gt;$\mathcal{P}$ is a state transition probability matrix&lt;/li&gt;&lt;li&gt;$\mathcal{R}$ is a reward function&lt;/li&gt;&lt;li&gt;$\gamma$ is a discount factor $\gamma \in [0, 1]$&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;action value: $Q(a) = \mathbb{E} [r \vert a] = \theta$&lt;/p&gt;&lt;h2 id="-epsilon-greedy-algorithm"&gt;$\epsilon$-Greedy Algorithm&lt;/h2&gt;</content:encoded></item></channel></rss>