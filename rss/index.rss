<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>TBD</title><description>Thoughts, stories and ideas.</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>TBD</title><link>http://localhost:2368/</link></image><generator>Ghost 2.2</generator><lastBuildDate>Mon, 05 Nov 2018 09:00:32 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Auction: Vickrey–Clarke–Groves (VCG) 
 or Generalized Second Price (GSP)?</title><description>&lt;p&gt;test&lt;/p&gt;</description><link>http://localhost:2368/autction-vcg-or-gsf/</link><guid isPermaLink="false">5bd15d1f1258b83180412192</guid><category>Random</category><dc:creator>Chenglin Lu</dc:creator><pubDate>Thu, 25 Oct 2018 06:16:57 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/10/rawpixel-1055781-unsplash.jpg" medium="image"/><content:encoded>&lt;img src="http://localhost:2368/content/images/2018/10/rawpixel-1055781-unsplash.jpg" alt="Auction: Vickrey–Clarke–Groves (VCG) 
 or Generalized Second Price (GSP)?"&gt;&lt;p&gt;test&lt;/p&gt;</content:encoded></item><item><title>The Multi-Armed Bandit Problem</title><description>&lt;h2 id="problem-setup"&gt;Problem Setup&lt;/h2&gt;&lt;p&gt;The multi-armed bandit problem can be described as a Markov decision process, a tuple $\langle \mathcal{X}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, with only one state.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;$\mathcal{X} = {x}$ is a finite set of states&lt;/li&gt;&lt;li&gt;$\mathcal{A}$ is a finite set of actions&lt;/li&gt;&lt;li&gt;$\mathcal{P}&lt;/li&gt;&lt;/ul&gt;</description><link>http://localhost:2368/the-multi-armed-bandit-problem/</link><guid isPermaLink="false">5bcc1aabf10170222eac70fa</guid><category>Reinforcement Learning</category><dc:creator>Chenglin Lu</dc:creator><pubDate>Sun, 21 Oct 2018 06:20:35 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/10/carl-raw-584973-unsplash.jpg" medium="image"/><content:encoded>&lt;h2 id="problem-setup"&gt;Problem Setup&lt;/h2&gt;&lt;img src="http://localhost:2368/content/images/2018/10/carl-raw-584973-unsplash.jpg" alt="The Multi-Armed Bandit Problem"&gt;&lt;p&gt;The multi-armed bandit problem can be described as a Markov decision process, a tuple $\langle \mathcal{X}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, with only one state.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;$\mathcal{X} = {x}$ is a finite set of states&lt;/li&gt;&lt;li&gt;$\mathcal{A}$ is a finite set of actions&lt;/li&gt;&lt;li&gt;$\mathcal{P}$ is a state transition probability matrix&lt;/li&gt;&lt;li&gt;$\mathcal{R}$ is a reward function, the immediate reward function $r: \mathcal{X} \times \mathcal{A} \to \mathbb{R}$&lt;/li&gt;&lt;li&gt;$\gamma$ is a discount factor $\gamma \in [0, 1]$. It is called undiscounted if $\gamma =1$.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;$\pi :  \mathcal{X} \to \mathcal{A}$ is the policy; $Q^\pi : \mathcal{X} \times \mathcal{A} \to \mathbb{R}$ is the action-value function.&lt;/p&gt;&lt;p&gt;$$\begin{align} Q^\pi(x, a) &amp;amp;= \mathbb{E} [ \sum_{t=0}^{\infty} \gamma^t R_{t+1}\vert x, a] \\ &amp;amp;= \mathbb{E} [  \sum_{t=0}^{\infty} R_{t+1}\vert x, a] &amp;amp; \text{let}\ \gamma = 1 \end{align}$$&lt;/p&gt;&lt;p&gt;The value function $V^\pi : \mathcal{X} \to \mathbb{R}$ is (when $\gamma=1$):&lt;/p&gt;&lt;p&gt;$$\begin{align} V^\pi(x) = \mathbb{E} [  \sum_{t=0}^{\infty} R_{t+1}\vert x] \quad  x \in \mathcal{X} \end{align}$$&lt;/p&gt;&lt;p&gt;The optimal value and action value function is:&lt;/p&gt;&lt;p&gt;$$\begin{align} V^*(x) = \sup_{a \in \mathcal{A}} Q^*(x, a) \quad  x \in \mathcal{X} \end{align}$$&lt;/p&gt;&lt;p&gt;We can minimize the total regret as a proxy to maximize the cumulative reward:&lt;/p&gt;&lt;h2 id="monte-carlo-method"&gt;Monte-Carlo Method&lt;/h2&gt;&lt;p&gt;The idea is that one can estimate the value of a state by computing sample means&lt;/p&gt;&lt;p&gt;$$\hat Q_{t}(x,a)=\frac {1} {N_t(x,a)}  \sum_{\tau=0}^{t} r_{\tau} \unicode{x1D7D9}_{a_t = a}$$&lt;/p&gt;&lt;p&gt;which can easily derive the so-called &lt;strong&gt;every-visit Monte-Carlo Method&lt;/strong&gt;:&lt;/p&gt;&lt;p&gt;$$\hat Q_{t+1}(x,a)=\hat Q_t(x,a)+\frac {1} {N_{t+1}(x,a)}(r_{t+1} - \hat Q_t(x,a)) \unicode{x1D7D9}_{a_t = a}$$&lt;/p&gt;&lt;h2 id="greedy-algorithm"&gt;Greedy Algorithm&lt;/h2&gt;&lt;h3 id="-epsilon-greedy-exploration-strategy-"&gt;$\epsilon$-greedy (exploration) strategy:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Let next action $a_{t+1} = \underset{a \in \mathcal{A}}{\arg\max} \hat{Q}_t(x, a)$ with probability $1-\epsilon$&lt;/li&gt;&lt;li&gt;Draw next action $a_{t+1}$ randomly with probability $\epsilon$&lt;/li&gt;&lt;/ul&gt;&lt;h3 id="boltzmann-exploration"&gt;Boltzmann exploration&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Draw next action $a_{t+1}$ from the multinomial distribution $\frac {exp(\beta \hat{Q}_t(x, a))} {\sum exp(\beta \hat{Q}_t(x, a))}$&lt;/li&gt;&lt;/ul&gt;&lt;iframe frameborder="0" scrolling="no" width="100%" src="http://localhost:2368/the-multi-armed-bandit-problem/tmp.html"&gt;&lt;/iframe&gt;&lt;p&gt;&lt;/p&gt;&lt;h1 id="references"&gt;References&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html"&gt;https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</content:encoded></item></channel></rss>