<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>苦しいことは繊細に映る高画質レンズで</title><description>Thoughts, stories and ideas.</description><link>https://blog.chenglin.lu/</link><image><url>https://blog.chenglin.lu/favicon.png</url><title>苦しいことは繊細に映る高画質レンズで</title><link>https://blog.chenglin.lu/</link></image><generator>Ghost 2.23</generator><lastBuildDate>Sun, 21 Jul 2019 20:08:34 GMT</lastBuildDate><atom:link href="https://blog.chenglin.lu/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Learnings from "The Book of Why"</title><description>&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="confounderandmediatorchapter4"&gt;Confounder and Mediator (Chapter 4)&lt;/h2&gt;
&lt;p&gt;Control for a mediator (e.g., add to a regression model) could block the effect between the treatment and the outcome.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You should only control for $Z$ if you have a &amp;quot;strong prior reason&amp;quot; ... This &amp;quot;strong prior reason&amp;quot; is nothing more&lt;/p&gt;&lt;/blockquote&gt;</description><link>https://blog.chenglin.lu/learnings-from-the-book-of-why/</link><guid isPermaLink="false">5d054ac765476d5db4678b0d</guid><dc:creator>Chenglin Lu</dc:creator><pubDate>Sat, 15 Jun 2019 19:47:06 GMT</pubDate><media:content url="https://blog.chenglin.lu/content/images/2019/07/emily-morter-8xAA0f9yQnE-unsplash.jpg" medium="image"/><content:encoded>&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="confounderandmediatorchapter4"&gt;Confounder and Mediator (Chapter 4)&lt;/h2&gt;
&lt;img src="https://blog.chenglin.lu/content/images/2019/07/emily-morter-8xAA0f9yQnE-unsplash.jpg" alt="Learnings from "The Book of Why""&gt;&lt;p&gt;Control for a mediator (e.g., add to a regression model) could block the effect between the treatment and the outcome.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You should only control for $Z$ if you have a &amp;quot;strong prior reason&amp;quot; ... This &amp;quot;strong prior reason&amp;quot; is nothing more or less than a causal assumption.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="simpsonsparadoxandlordsparadoxchapter6"&gt;Simpson's Paradox and Lord's Paradox (Chapter 6)&lt;/h2&gt;
&lt;p&gt;Change in the causal assumption should affect the way we formulate the effect between the treatment and the outcome.&lt;/p&gt;
&lt;!--kg-card-end: markdown--&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="reference"&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Pearl, Judea and Dana Mackenzie. &lt;em&gt;The Book of Why: The New Science of Cause and Effect&lt;/em&gt;. Basic Books, 2018.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--kg-card-end: markdown--&gt;</content:encoded></item><item><title>The Multi-Armed Bandit Problem</title><description>&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="problemsetup"&gt;Problem Setup&lt;/h2&gt;
&lt;p&gt;The multi-armed bandit problem can be described as a Markov decision process, a tuple $\langle \mathcal{X}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, with only one state.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{X} = {x}$ is a finite set of states&lt;/li&gt;
&lt;li&gt;$\mathcal{A}$ is a finite set of actions&lt;/li&gt;
&lt;li&gt;$\mathcal{P}&lt;/li&gt;&lt;/ul&gt;</description><link>https://blog.chenglin.lu/the-multi-armed-bandit-problem/</link><guid isPermaLink="false">5d00ba2165476d5db4678aad</guid><dc:creator>Chenglin Lu</dc:creator><pubDate>Sat, 20 Oct 2018 08:38:00 GMT</pubDate><media:content url="https://blog.chenglin.lu/content/images/2019/06/carl-raw-584973-unsplash-1.jpg" medium="image"/><content:encoded>&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="problemsetup"&gt;Problem Setup&lt;/h2&gt;
&lt;img src="https://blog.chenglin.lu/content/images/2019/06/carl-raw-584973-unsplash-1.jpg" alt="The Multi-Armed Bandit Problem"&gt;&lt;p&gt;The multi-armed bandit problem can be described as a Markov decision process, a tuple $\langle \mathcal{X}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, with only one state.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{X} = {x}$ is a finite set of states&lt;/li&gt;
&lt;li&gt;$\mathcal{A}$ is a finite set of actions&lt;/li&gt;
&lt;li&gt;$\mathcal{P}$ is a state transition probability matrix&lt;/li&gt;
&lt;li&gt;$\mathcal{R}$ is a reward function, the immediate reward function $r: \mathcal{X} \times \mathcal{A} \to \mathbb{R}$&lt;/li&gt;
&lt;li&gt;$\gamma$ is a discount factor $\gamma \in [0, 1]$. It is called undiscounted if $\gamma=1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\pi :  \mathcal{X} \to \mathcal{A}$ is the policy; $Q^\pi : \mathcal{X} \times \mathcal{A} \to \mathbb{R}$ is the action-value function.&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{align}&lt;br&gt;
Q^\pi(x, a)&lt;br&gt;
&amp;amp;= \mathbb{E} [\sum_{t=0}^{\infty} \gamma^t R_{t+1}\vert x, a] \newline&lt;br&gt;
&amp;amp;= \mathbb{E} [\sum_{t=0}^{\infty} R_{t+1}\vert x, a] &amp;amp; \text{let}\ \gamma = 1&lt;br&gt;
\end{align}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The value function $V^\pi : \mathcal{X} \to \mathbb{R}$ is (when $\gamma=1$):&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{align}&lt;br&gt;
V^\pi(x) = \mathbb{E} [\sum_{t=0}^{\infty} R_{t+1}\vert x] \quad x \in \mathcal{X}&lt;br&gt;
\end{align}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;The optimal value and action value function is:&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;
\begin{align}&lt;br&gt;
V^*(x) = \sup_{a \in \mathcal{A}} Q^*(x, a) \quad x \in \mathcal{X}&lt;br&gt;
\end{align}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;We can minimize the total regret as a proxy to maximize the cumulative reward:&lt;/p&gt;
&lt;h2 id="montecarlomethod"&gt;Monte-Carlo Method&lt;/h2&gt;
&lt;p&gt;The idea is that one can estimate the value of a state by computing sample means&lt;br&gt;
$$&lt;br&gt;
\hat Q_{t}(x,a)=\frac {1} {N_t(x,a)}  \sum_{\tau=0}^{t} r_{\tau} \unicode{x1D7D9}_{a_t = a}&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;which can easily derive the so-called &lt;strong&gt;every-visit Monte-Carlo Method&lt;/strong&gt;:&lt;br&gt;
$$&lt;br&gt;
\hat Q_{t+1}(x,a)=\hat Q_t(x,a)+\frac {1} {N_{t+1}(x,a)}(r_{t+1} - \hat Q_t(x,a)) \unicode{x1D7D9}_{a_t = a}&lt;br&gt;
$$&lt;/p&gt;
&lt;h2 id="greedyalgorithm"&gt;Greedy Algorithm&lt;/h2&gt;
&lt;h3 id="epsilongreedyexplorationstrategy"&gt;$\epsilon$-greedy (exploration) strategy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Let next action $a_{t+1} = \underset{a \in \mathcal{A}}{\arg\max} \hat{Q}_t(x, a)$ with probability $1-\epsilon$&lt;/li&gt;
&lt;li&gt;Draw next action $a_{t+1}$ randomly with probability $\epsilon$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="boltzmannexploration"&gt;Boltzmann exploration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Draw next action $a_{t+1}$ from the multinomial distribution $\frac {exp(\beta \hat{Q}_t(x, a))} {\sum exp(\beta \hat{Q}_t(x, a))}$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html"&gt;https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!--kg-card-end: markdown--&gt;</content:encoded></item></channel></rss>